class FileStructure:
    """
    File structure settings
    """
    DEPENDENCY_PATH = "./dependencies"
    DATASET_PATH = "../dataset/projects"
    CODE_INFO_PATH = "../dataset/project_index"
    BASELINE_PATH = "../evaluation/baseline"
    # "<project>" will be replaced by the project name in the execution process
    PROMPT_PATH = "../evaluation/<project>/context+prompts"
    FIX_PATH = "../evaluation/<project>/fix"
    RESPONSE_PATH = "../evaluation/<project>/responses"
    TESTCLASSS_PATH = "../evaluation/<project>/test_classes"
    REPORT_PATH = "../evaluation/<project>/reports"

class LLMSettings:
    """
    LLM settings
    """
    MODEL = "gpt-4o-mini"
    API_ACCOUNTS = [
        {   
            "base_url":"",
            "api_key":"xxx",
        }
    ]

class TaskSettings:
    """
    Task settings
    """
    # project selection
    # for projects and cases list, use empty list [] to select all
    PROJECTS = ['batch-processing-gateway', 'commons-cli', 'commons-codec', 'commons-collections', 'commons-csv', 'datafaker', 'gson', 'jdom2', 'ruler', 'windward']
    # test class selection, list of id in dataset, e.g. ["SparkPodSpec_copyFrom"]. use empty list [] to select all
    CASES_LIST = []
    # prompt selection
    PROMPT_LIST = ['condition','io','exception']
    # if True, generate test cases first, then generate test functions; otherwise, generate test functions directly
    CASE_THEN_CODE = True
    SAVE_INTER_RESULT = True
    COMPILE_TEST = True
    MAX_WORKERS = 8 # LLM API concurrency 
    FIX_TRIES = 3 # Maximum retries for fixing test cases
    SIM_TOP_K = "10" # top k for similarity search

class BaseLine:
    """
    BaseLine results
    """
    BASELINES = ["ChatUniTest"] # ["HITS", "ChatUniTest"]
    BASELINE_PATH = "../evaluation/baseline"
    HITS_RESULTS = ""